{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Get the directory of the current script\n",
    "try:\n",
    "    # This will work if the script is being run as a file\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Use the current working directory if __file__ is not defined\n",
    "    current_dir = os.getcwd()\n",
    "# Move up one directory level from EDA to the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Construct the path to the xdrive folder\n",
    "xdrive_path = os.path.join(parent_dir, 'xdrive')\n",
    "# Add the xdrive path to sys.path\n",
    "sys.path.append(xdrive_path)\n",
    "\n",
    "featureeng_path = os.path.join(parent_dir, 'Feature Engineering')\n",
    "# Add the xdrive path to sys.path\n",
    "sys.path.append(featureeng_path)\n",
    "\n",
    "# Construct the path to the xdrive folder\n",
    "xdrive_path = os.path.join(parent_dir, 'Pipeline')\n",
    "# Add the xdrive path to sys.path\n",
    "sys.path.append(xdrive_path)\n",
    "from build_a_pipeline import build_pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "from build_a_pipeline import build_pipeline_P0_foward_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veron\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_performance = []\n",
    "#x_train, x_val, X_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(target_column = 'P0',drop_features=False)\n",
    "x_train, x_val, X_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(drop_features=True, \n",
    "                                                                                remove_outliers = True, \n",
    "                                                                                percentage_of_outliers=0.01, \n",
    "                                                                                deal_with_skewness = True) #drop_features = False\n",
    "# x_train = x_train.drop(columns = ['Average_Settlement_Price_SE/CW(MWh)', 'Min_Settlement_Price_SE(MWh)'])\n",
    "# x_val = x_val.drop(columns = ['Average_Settlement_Price_SE/CW(MWh)', 'Min_Settlement_Price_SE(MWh)'])\n",
    "# X_test = X_test.drop(columns = ['Average_Settlement_Price_SE/CW(MWh)', 'Min_Settlement_Price_SE(MWh)'])\n",
    "datetime_index = x_val.index\n",
    "# Convert x_train and x_val to NumPy arrays\n",
    "x_val_original = x_val\n",
    "X_test_original = X_test\n",
    "x_train = x_train.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "# Reshape x_train and x_val to have the required 3D shape\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])  # [samples, time_steps=1, features]\n",
    "x_val = x_val.reshape(x_val.shape[0], 1, x_val.shape[1])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veron\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 30350.6074 - root_mean_squared_error: 173.8955 - val_loss: 2197.4275 - val_root_mean_squared_error: 46.8767\n",
      "Epoch 2/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11196.2676 - root_mean_squared_error: 105.2156 - val_loss: 11147.5977 - val_root_mean_squared_error: 105.5822\n",
      "Epoch 3/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6691.0664 - root_mean_squared_error: 81.4461 - val_loss: 17691.9199 - val_root_mean_squared_error: 133.0110\n",
      "Epoch 4/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2761.6912 - root_mean_squared_error: 52.2503 - val_loss: 16718.5254 - val_root_mean_squared_error: 129.3001\n",
      "Epoch 5/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1236.3414 - root_mean_squared_error: 34.8854 - val_loss: 18933.7441 - val_root_mean_squared_error: 137.5999\n",
      "Epoch 6/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1109.9252 - root_mean_squared_error: 33.2086 - val_loss: 16159.8057 - val_root_mean_squared_error: 127.1212\n",
      "Epoch 7/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1216.2050 - root_mean_squared_error: 34.7699 - val_loss: 16636.9531 - val_root_mean_squared_error: 128.9843\n",
      "Epoch 8/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 838.6234 - root_mean_squared_error: 28.9069 - val_loss: 16551.9023 - val_root_mean_squared_error: 128.6542\n",
      "Epoch 9/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 735.3526 - root_mean_squared_error: 27.0313 - val_loss: 18465.7852 - val_root_mean_squared_error: 135.8889\n",
      "Epoch 10/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 810.5331 - root_mean_squared_error: 28.3313 - val_loss: 20684.5703 - val_root_mean_squared_error: 143.8213\n",
      "Epoch 11/120\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 776.8644 - root_mean_squared_error: 27.7630 - val_loss: 19014.7598 - val_root_mean_squared_error: 137.8940\n",
      "Final train RMSE: 26.379438400268555\n",
      "Final validation RMSE: 137.89401245117188\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "R-squared: -38.19302594414393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Build the LSTM model\n",
    "multivariate_lstm = Sequential([\n",
    "    LSTM(195, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    TimeDistributed(Dense(95, activation='selu')), \n",
    "    Dropout(0.1),\n",
    "    Flatten(),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('multivariate_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Optimizer with adjusted learning rate\n",
    "optimizer = Adam(learning_rate=6e-3, amsgrad=True)\n",
    "\n",
    "# Loss function and metric\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Compile the model\n",
    "multivariate_lstm.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "# Train the model (make sure you pass both x_val and y_val)\n",
    "history = multivariate_lstm.fit(x_train, y_train, epochs=120,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Optionally, print the final RMSE\n",
    "train_rmse = history.history['root_mean_squared_error'][-1]\n",
    "val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "print(f\"Final train RMSE: {train_rmse}\")\n",
    "print(f\"Final validation RMSE: {val_rmse}\")\n",
    "model_performance.append({\n",
    "    'Model': '1st Model',\n",
    "    'LayerExtra': 'TimeDistributed, LSTM',\n",
    "    'Train RMSE': train_rmse,\n",
    "    'Validation RMSE': val_rmse\n",
    "})\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(x_val)\n",
    "r_squared = r2_score(y_val, y_pred)\n",
    "print(f\"R-squared: {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the pipeline with no feature selection dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_val, X_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(drop_features=False, \n",
    "                                                                                remove_outliers = True, \n",
    "                                                                                percentage_of_outliers=0.01, \n",
    "                                                                                train_size = 0.75, \n",
    "                                                                                val_size = 0.15, \n",
    "                                                                                deal_with_skewness = True) #drop_features = False\n",
    "\n",
    "# Convert x_train and x_val to NumPy arrays\n",
    "x_train = x_train.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "# Reshape x_train and x_val to have the required 3D shape\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1])  # [samples, time_steps=1, features]\n",
    "x_val = x_val.reshape(x_val.shape[0], 1, x_val.shape[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "# Build the LSTM model\n",
    "multivariate_lstm = Sequential([\n",
    "    LSTM(195, input_shape=(x_train.shape[1], x_train.shape[2]), kernel_initializer=GlorotUniform(seed=42), return_sequences=True),\n",
    "    TimeDistributed(Dense(95, activation='selu', kernel_initializer=GlorotUniform(seed=42))), \n",
    "    Flatten(),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('multivariate_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Optimizer with adjusted learning rate\n",
    "optimizer = Adam(learning_rate=6e-3, amsgrad=True, clipnorm=1.0)\n",
    "#optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "# Loss function and metric\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Compile the model\n",
    "multivariate_lstm.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "# Train the model (make sure you pass both x_val and y_val)\n",
    "history = multivariate_lstm.fit(x_train, y_train, epochs=120,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Optionally, print the final RMSE\n",
    "train_rmse = history.history['root_mean_squared_error'][-1]\n",
    "val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "print(f\"Final train RMSE: {train_rmse}\")\n",
    "print(f\"Final validation RMSE: {val_rmse}\")\n",
    "model_performance.append({\n",
    "    'Model': '1st Model',\n",
    "    'LayerExtra': 'TimeDistributed, LSTM',\n",
    "    'Train RMSE': train_rmse,\n",
    "    'Validation RMSE': val_rmse\n",
    "})\n",
    "\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(x_val)\n",
    "r_squared = r2_score(y_val, y_pred)\n",
    "print(f\"R-squared: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "\n",
    "# Build the LSTM multivariate_lstm\n",
    "multivariate_lstm = Sequential([\n",
    "    LSTM(200, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True),\n",
    "    TimeDistributed(Dense(100, activation='relu')), \n",
    "    Flatten(),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# ModelCheckpoint to save the best multivariate_lstm based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('multivariate_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Optimizer with adjusted learning rate\n",
    "optimizer = Adam(learning_rate=6e-3, amsgrad=True)\n",
    "\n",
    "# Loss function and metric\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Compile the multivariate_lstm\n",
    "multivariate_lstm.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "# Train the multivariate_lstm (make sure you pass both x_val and y_val)\n",
    "history = multivariate_lstm.fit(x_train, y_train, epochs=120,\n",
    "                                validation_data=(x_val, y_val),\n",
    "                                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Optionally, print the final RMSE\n",
    "train_rmse = history.history['root_mean_squared_error'][-1]\n",
    "val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "print(f\"Final train RMSE: {train_rmse}\")\n",
    "print(f\"Final validation RMSE: {val_rmse}\")\n",
    "\n",
    "model_performance.append({\n",
    "    'multivariate_lstm': multivariate_lstm,\n",
    "    'Train RMSE': train_rmse,\n",
    "    'Validation RMSE': val_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "predictions_df = pd.DataFrame(y_pred, index=datetime_index, columns=['Predicted'])\n",
    "predictions_df = predictions_df.sort_index()\n",
    "y_val = y_val.sort_index()\n",
    "\n",
    "# Plotting y_actual vs y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_val.index, y_val, label='Actual', marker='o')\n",
    "plt.plot(y_val.index, predictions_df['Predicted'], label='Predicted', marker='x')\n",
    "plt.title('Actual vs Predicted Values MultiVariateLSTM')\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Rotate the x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming `df` is the DataFrame with 42 columns and `x_val_original` contains 'Average_Settlement_Price_SE/CW(MWh)'\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the columns from df\n",
    "for column in predictions_df.columns:\n",
    "    fig.add_trace(go.Scatter(x=predictions_df.index, y=predictions_df[column], mode='lines', name=column))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=y_val.index, y=y_val, mode='lines', name='Actual'))\n",
    "fig.add_trace(go.Scatter(x=x_val_original.index, y=x_val_original['Average_Settlement_Price_SE/CW(MWh)'], mode='lines', name='Average_Settlement_Price_SE/CW(MWh)'))\n",
    "\n",
    "# Plot the 'Average_Settlement_Price_SE/CW(MWh)' from x_val_original with a secondary y-axis\n",
    "fig.add_trace(go.Scatter(x=x_val_original.index, \n",
    "                         y=x_val_original['Hydro_Inflow_NE(MWavg)'], \n",
    "                         mode='lines', \n",
    "                         name='Hydro_Inflow_NE(MWavg)', \n",
    "                         yaxis='y2'))  # Use the second y-axis\n",
    "\n",
    "# Update layout to add a secondary y-axis\n",
    "fig.update_layout(\n",
    "    title='Visualization of All Columns in the Dataset',\n",
    "    xaxis_title=\"DateTime or Index\",\n",
    "    yaxis_title=\"Values (df)\",\n",
    "    yaxis2=dict(\n",
    "        title=\"Average Settlement Price (x_val_original)\",\n",
    "        overlaying=\"y\",  # Overlay the second y-axis on the primary y-axis\n",
    "        side=\"right\",  # Place the secondary y-axis on the right\n",
    "        showgrid=False  # Optionally hide the grid for the secondary y-axis\n",
    "    ),\n",
    "    xaxis_tickangle=45,  # Rotate x-axis labels for better visibility\n",
    "    template=\"plotly_dark\",  # Optional: change the theme\n",
    "    width=1200,  # Optional: adjust plot width\n",
    "    height=600,  # Optional: adjust plot height\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL TESTING\n",
    "- training on val and train\n",
    "- testing on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(drop_features=False, \n",
    "                                                                                remove_outliers = True, \n",
    "                                                                                percentage_of_outliers=0.01, \n",
    "                                                                                train_size = 0.75, \n",
    "                                                                                val_size = 0.15,\n",
    "                                                                                 deal_with_skewness = True) #drop_features = False\n",
    "datetime_index = x_test.index\n",
    "#save these values for later\n",
    "X_test_last= X_test.sort_index()\n",
    "X_test_last = X_test_last.tail(10)\n",
    "datetime_index_final = X_test_last.index\n",
    "X_test_last = X_test_last.to_numpy()  # Convert DataFrame to NumPy array\n",
    "X_test_last = X_test_last.reshape(X_test_last.shape[0], 1, X_test_last.shape[1])\n",
    "y_test_last = y_test.sort_index()\n",
    "y_test_last = y_test_last.tail(10)\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# merge train and validation\n",
    "x_train_combined = np.vstack([x_train, x_val])  # Combine x_train and x_val\n",
    "y_train_combined = np.concatenate([y_train, y_val])  # Combine y_train and y_val\n",
    "\n",
    "#reshape\n",
    "x_train = x_train_combined.reshape(x_train_combined.shape[0], 1, x_train_combined.shape[1])  # [samples, time_steps=1, features]\n",
    "# Test data\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "# Build the LSTM model\n",
    "multivariate_lstm = Sequential([\n",
    "    LSTM(105, input_shape=(x_train.shape[1], x_train.shape[2]), kernel_initializer=GlorotUniform(seed=42), return_sequences=True),\n",
    "    Dropout(0.4),\n",
    "    TimeDistributed(Dense(75, activation='selu', kernel_initializer=GlorotUniform(seed=42))), \n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('multivariate_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Optimizer with adjusted learning rate\n",
    "optimizer = Adam(learning_rate=6e-3, amsgrad=True, clipnorm=1.0)\n",
    "#optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "# Loss function and metric\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Compile the model\n",
    "multivariate_lstm.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "history = multivariate_lstm.fit(x_train, y_train_combined, epochs=120,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Optionally, print the final RMSE\n",
    "train_rmse = history.history['root_mean_squared_error'][-1]\n",
    "val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "print(f\"Final train RMSE: {train_rmse}\")\n",
    "print(f\"Final test RMSE: {val_rmse}\")\n",
    "model_performance.append({\n",
    "    'Model': '1st Model',\n",
    "    'LayerExtra': 'TimeDistributed, LSTM',\n",
    "    'Train RMSE': train_rmse,\n",
    "    'Validation RMSE': val_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "predictions_df = pd.DataFrame(y_pred, index=datetime_index, columns=['Predicted'])\n",
    "y_test_df = pd.DataFrame(y_test, index=datetime_index, columns=['Actual'])\n",
    "predictions_df = predictions_df.sort_index()\n",
    "y_test_df = y_test_df.sort_index()\n",
    "\n",
    "# Plotting y_actual vs y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(predictions_df.index, y_test_df['Actual'], label='Actual', marker='o')\n",
    "plt.plot(y_test_df.index, predictions_df['Predicted'], label='Predicted', marker='x')\n",
    "plt.title('Actual vs Predicted Values MultiVariateLSTM on TEST')\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Rotate the x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last 10 values prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(X_test_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "y_pred = y_pred.flatten()\n",
    "predictions_df = pd.DataFrame(y_pred, index=datetime_index_final, columns=['Predicted'])\n",
    "predictions_df = predictions_df.sort_index()\n",
    "\n",
    "# Plotting y_actual vs y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_last.index, y_test_last, label='Actual', marker='o')\n",
    "plt.plot(predictions_df.index, predictions_df['Predicted'], label='Predicted', marker='x')\n",
    "plt.title('Actual vs Predicted Values MultiVariateLSTM on TEST')\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Rotate the x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is not performing on September or October 2024, but lets look at beginning of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_val, x_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(target_column='P0', drop_features=False)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = build_pipeline_P0_foward_price(drop_features=False, \n",
    "                                                                                remove_outliers = True, \n",
    "                                                                                percentage_of_outliers=0.05, \n",
    "                                                                                train_size = 0.70, \n",
    "                                                                                val_size = 0.05) #drop_features = False\n",
    "x_test.index = pd.to_datetime(x_test.index)\n",
    "y_test.index = pd.to_datetime(y_test.index)\n",
    "datetime_index = x_test.index\n",
    "#save these values for later\n",
    "X_test_last= X_test.sort_index()\n",
    "X_test_last = x_test.loc['2024-02-01':'2024-05-30']\n",
    "datetime_index_final = X_test_last.index\n",
    "X_test_last = X_test_last.to_numpy()  # Convert DataFrame to NumPy array\n",
    "X_test_last = X_test_last.reshape(X_test_last.shape[0], 1, X_test_last.shape[1])\n",
    "y_test_last = y_test.sort_index()\n",
    "y_test_last = y_test_last.loc['2024-02-01':'2024-05-30']\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# merge train and validation\n",
    "x_train_combined = np.vstack([x_train, x_val])  # Combine x_train and x_val\n",
    "y_train_combined = np.concatenate([y_train, y_val])  # Combine y_train and y_val\n",
    "\n",
    "#reshape\n",
    "x_train = x_train_combined.reshape(x_train_combined.shape[0], 1, x_train_combined.shape[1])  # [samples, time_steps=1, features]\n",
    "# Test data\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "# Build the LSTM model\n",
    "multivariate_lstm = Sequential([\n",
    "    LSTM(250, input_shape=(x_train.shape[1], x_train.shape[2]), kernel_initializer=GlorotUniform(seed=42), return_sequences=True),\n",
    "    Dropout(0.4),\n",
    "    TimeDistributed(Dense(200, activation='selu', kernel_initializer=GlorotUniform(seed=42))), \n",
    "    Dropout(0.2),\n",
    "    TimeDistributed(Dense(200, activation='selu', kernel_initializer=GlorotUniform(seed=42))), \n",
    "    Dropout(0.2),\n",
    "    TimeDistributed(Dense(100, activation='selu', kernel_initializer=GlorotUniform(seed=42))),\n",
    "    Flatten(),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# ModelCheckpoint to save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('multivariate_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Optimizer with adjusted learning rate\n",
    "optimizer = Adam(learning_rate=6e-3, amsgrad=True, clipnorm=1.0)\n",
    "#optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "# Loss function and metric\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "# Compile the model\n",
    "multivariate_lstm.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
    "\n",
    "history = multivariate_lstm.fit(x_train, y_train_combined, epochs=120,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Optionally, print the final RMSE\n",
    "train_rmse = history.history['root_mean_squared_error'][-1]\n",
    "val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "print(f\"Final train RMSE: {train_rmse}\")\n",
    "print(f\"Final test RMSE: {val_rmse}\")\n",
    "model_performance.append({\n",
    "    'Model': '1st Model',\n",
    "    'LayerExtra': 'TimeDistributed, LSTM',\n",
    "    'Train RMSE': train_rmse,\n",
    "    'Validation RMSE': val_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('multivariate_lstm.keras')\n",
    "y_pred = best_model.predict(X_test_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "predictions_df = pd.DataFrame(y_pred, index=datetime_index_final, columns=['Predicted'])\n",
    "y_test_df = pd.DataFrame(y_test_last, index=datetime_index_final, columns=['Actual'])\n",
    "predictions_df = predictions_df.sort_index()\n",
    "y_test_df = y_test_df.sort_index()\n",
    "\n",
    "# Plotting y_actual vs y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datetime_index_final, y_test_last, label='Actual', marker='o')\n",
    "plt.plot(y_test_df.index, predictions_df['Predicted'], label='Predicted', marker='x')\n",
    "plt.title('Actual vs Predicted Values MultiVariateLSTM on TEST')\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Rotate the x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
